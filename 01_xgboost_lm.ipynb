{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: array([0, 0, 0, 0, 0], dtype=uint8),\n",
       "  1: array([0, 0, 0, 0, 1], dtype=uint8),\n",
       "  2: array([0, 0, 0, 1, 0], dtype=uint8),\n",
       "  3: array([0, 0, 0, 1, 1], dtype=uint8),\n",
       "  4: array([0, 0, 1, 0, 0], dtype=uint8),\n",
       "  5: array([0, 0, 1, 0, 1], dtype=uint8),\n",
       "  6: array([0, 0, 1, 1, 0], dtype=uint8),\n",
       "  7: array([0, 0, 1, 1, 1], dtype=uint8),\n",
       "  8: array([0, 1, 0, 0, 0], dtype=uint8),\n",
       "  9: array([0, 1, 0, 0, 1], dtype=uint8),\n",
       "  10: array([0, 1, 0, 1, 0], dtype=uint8),\n",
       "  11: array([0, 1, 0, 1, 1], dtype=uint8),\n",
       "  12: array([0, 1, 1, 0, 0], dtype=uint8),\n",
       "  13: array([0, 1, 1, 0, 1], dtype=uint8),\n",
       "  14: array([0, 1, 1, 1, 0], dtype=uint8),\n",
       "  15: array([0, 1, 1, 1, 1], dtype=uint8),\n",
       "  16: array([1, 0, 0, 0, 0], dtype=uint8),\n",
       "  17: array([1, 0, 0, 0, 1], dtype=uint8),\n",
       "  18: array([1, 0, 0, 1, 0], dtype=uint8),\n",
       "  19: array([1, 0, 0, 1, 1], dtype=uint8),\n",
       "  20: array([1, 0, 1, 0, 0], dtype=uint8),\n",
       "  21: array([1, 0, 1, 0, 1], dtype=uint8),\n",
       "  22: array([1, 0, 1, 1, 0], dtype=uint8),\n",
       "  23: array([1, 0, 1, 1, 1], dtype=uint8),\n",
       "  24: array([1, 1, 0, 0, 0], dtype=uint8),\n",
       "  25: array([1, 1, 0, 0, 1], dtype=uint8),\n",
       "  26: array([1, 1, 0, 1, 0], dtype=uint8)},\n",
       " {'a': 0,\n",
       "  'b': 1,\n",
       "  'c': 2,\n",
       "  'd': 3,\n",
       "  'e': 4,\n",
       "  'f': 5,\n",
       "  'g': 6,\n",
       "  'h': 7,\n",
       "  'i': 8,\n",
       "  'j': 9,\n",
       "  'k': 10,\n",
       "  'l': 11,\n",
       "  'm': 12,\n",
       "  'n': 13,\n",
       "  'o': 14,\n",
       "  'p': 15,\n",
       "  'q': 16,\n",
       "  'r': 17,\n",
       "  's': 18,\n",
       "  't': 19,\n",
       "  'u': 20,\n",
       "  'v': 21,\n",
       "  'w': 22,\n",
       "  'x': 23,\n",
       "  'y': 24,\n",
       "  'z': 25,\n",
       "  '0': 26},\n",
       " {0: 'a',\n",
       "  1: 'b',\n",
       "  2: 'c',\n",
       "  3: 'd',\n",
       "  4: 'e',\n",
       "  5: 'f',\n",
       "  6: 'g',\n",
       "  7: 'h',\n",
       "  8: 'i',\n",
       "  9: 'j',\n",
       "  10: 'k',\n",
       "  11: 'l',\n",
       "  12: 'm',\n",
       "  13: 'n',\n",
       "  14: 'o',\n",
       "  15: 'p',\n",
       "  16: 'q',\n",
       "  17: 'r',\n",
       "  18: 's',\n",
       "  19: 't',\n",
       "  20: 'u',\n",
       "  21: 'v',\n",
       "  22: 'w',\n",
       "  23: 'x',\n",
       "  24: 'y',\n",
       "  25: 'z',\n",
       "  26: '0'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The idea is to encode alphabets into numbers\n",
    "alphabets = 'abcdefghijklmnopqrstuvwxyz0'\n",
    "\n",
    "# assign a number to each alphabet\n",
    "alphabet2int = dict(zip(alphabets, range(0, len(alphabets)+1)))\n",
    "int2alphabet = dict(zip(range(0, len(alphabets)+1), alphabets))\n",
    "\n",
    "# make a 2 bit encoding for int\n",
    "int2binary = {}\n",
    "binary = np.unpackbits(np.array([range(len(alphabets))], dtype=np.uint8).T, axis=1)[:, -5:]\n",
    "\n",
    "for i in range(len(alphabets)):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "int2binary, alphabet2int, int2alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training corpus; predict next character given last two characters\n",
    "\n",
    "# model takes \"xgb\" nad predicts \"o\" \n",
    "# thus a training sample is (x, y) = (\"xgb\", \"o\") as numbers\n",
    "\n",
    "# training data\n",
    "\n",
    "MAXLEN_PER_WORD = len(\"xgboost0\")\n",
    "N_SAMPLES = 1000\n",
    "N_CHARS_IN = 4\n",
    "N_CHARS_OUT = 1\n",
    "N_LABELS = len(int2binary[0])\n",
    "\n",
    "corpus = \"\"\n",
    "for i in range(N_SAMPLES):\n",
    "    corpus += \"xgboost0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 0xgb -> o\n",
      "Int: [26, 23, 6, 1] -> 14\n",
      "Binary: [array([1, 1, 0, 1, 0], dtype=uint8), array([1, 0, 1, 1, 1], dtype=uint8), array([0, 0, 1, 1, 0], dtype=uint8), array([0, 0, 0, 0, 1], dtype=uint8)] -> [0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# chunking the corpus into training samples\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(0, len(corpus) - MAXLEN_PER_WORD, 1):\n",
    "    \n",
    "    x_text = corpus[i:i+N_CHARS_IN]\n",
    "    y_text = corpus[i+N_CHARS_IN]\n",
    "\n",
    "    x_text_int = [alphabet2int[c] for c in x_text]\n",
    "    y_text_int = alphabet2int[y_text]\n",
    "\n",
    "    # binary encoding\n",
    "    x_train.append([int2binary[c] for c in x_text_int])\n",
    "    y_train.append(int2binary[y_text_int])\n",
    "\n",
    "print(f\"Text: {x_text} -> {y_text}\")\n",
    "print(f\"Int: {x_text_int} -> {y_text_int}\")\n",
    "print(f\"Binary: {x_train[-1]} -> {y_train[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7992, 20)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# reshape X from (N_SAMPLES, N_CHARS_IN, N_LABELS) -> (N_SAMPLES, N_CHARS_IN * N_LABELS)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "print(f\"X shape: {x_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=&#x27;multi_output_tree&#x27;, n_estimators=10000,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=&#x27;multi_output_tree&#x27;, n_estimators=10000,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy='multi_output_tree', n_estimators=10000,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method=\"hist\", \n",
    "    multi_strategy=\"multi_output_tree\",\n",
    "    objective=\"binary:logistic\",\n",
    "    n_estimators=10000,\n",
    "    )\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: xgbo -> o\n",
      "Input: gboo -> s\n",
      "Input: boos -> t\n",
      "Input: oost -> 0\n",
      "Input: ost0 -> x\n",
      "Input: st0x -> g\n",
      "Input: t0xg -> b\n",
      "Input: 0xgb -> o\n",
      "Input: xgbo -> o\n",
      "Input: gboo -> s\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "input_text = \"gboo\"\n",
    "\n",
    "def predict_next_char(input_text, model):\n",
    "    input_text_int = [alphabet2int[c] for c in input_text]\n",
    "    input_text_binary = [int2binary[c] for c in input_text_int]\n",
    "    input_text_binary = np.array(input_text_binary).reshape(1, -1)\n",
    "\n",
    "    pred = model.predict(input_text_binary)\n",
    "    # pad with zeros\n",
    "    pred = [[0]*3 + list(p) for p in pred]\n",
    "    pred = np.array(pred).reshape(-1).astype(bool)\n",
    "\n",
    "    # convert binary to int\n",
    "    pred_int = np.packbits(pred)\n",
    "    pred_int = int(pred_int)\n",
    "\n",
    "    if pred_int > 26:\n",
    "        pred_int = 26\n",
    "    # convert int to alphabet\n",
    "    pred_text = int2alphabet[pred_int]\n",
    "    return pred_text\n",
    "\n",
    "input_samples = [corpus[i:i+N_CHARS_IN] for i in range(0, len(corpus) - MAXLEN_PER_WORD, 1)][:10]\n",
    "\n",
    "for input_text in input_samples:\n",
    "    print(f\"Input: {input_text} -> {predict_next_char(input_text, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
